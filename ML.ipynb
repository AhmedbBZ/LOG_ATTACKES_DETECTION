{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"generated_log_dataset_finale.csv\")\n",
    "\n",
    "def Type_Class (x):\n",
    "  if (x in ['Brute Force Attack', 'SQL Injection', 'Cross-Site Scripting (XSS)' ,'Distributed Denial of Service (DDoS)' ,'Malware and Ransomware','Phishing and Credential Harvesting' ,'Privilege Escalation','Remote Code Execution (RCE)', 'Data Exfiltration','Insider Threats', 'Directory Traversal' ,'Cross-Site Request Forgery (CSRF)','Man-in-the-Middle (MITM)', 'Zero-Day Exploit' 'Phishing via Malicious Links'] ):\n",
    "    return 1\n",
    "  else :\n",
    "      return 0\n",
    "\n",
    "def test_data():\n",
    "  test_data=[]\n",
    "  for index,row in df.iterrows():\n",
    "    log=row[\"log\"]\n",
    "    log_class=row[\"class\"]\n",
    "    test_data.append((log,Type_Class(log_class)))\n",
    "  return test_data\n",
    "\n",
    "# 1. Load the saved model\n",
    "clf = joblib.load('model.pkl')  # Replace 'your_model.pkl' with your actual model file path\n",
    "\n",
    "# 2. Load the vectorizer (make sure it's the same vectorizer used in training)\n",
    "vectorizer = joblib.load('vectorizer.pkl')  # Replace 'your_vectorizer.pkl' with your actual vectorizer file path\n",
    "\n",
    "# 3. Prepare the test data (use the same format as the training data)\n",
    "# Example test data (replace with your actual test data)\n",
    "\n",
    "# Convert the list into a pandas DataFrame\n",
    "\n",
    "\n",
    "# Split the test data into features (X) and labels (y)\n",
    "X_test = df[\"log\"]\n",
    "y_test = df[\"class\"]\n",
    "\n",
    "# 4. Transform the test data using the loaded vectorizer\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# 5. Make predictions on the test set using the loaded model\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# 6. Print the predictions\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# 7. Evaluate the model's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# 8. Calculate and print Recall and F1 Score\n",
    "# Calculate and print Recall and F1 Score with multiclass support\n",
    "recall = recall_score(y_test, y_pred, average='weighted')  # Use 'weighted', 'micro', or 'macro'\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')  # Use 'weighted', 'micro', or 'macro'\n",
    "\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"/content/generated_log_dataset_finale.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report\n",
    "list_model=[]\n",
    "def ml_models(X_train, X_test, y_train, y_test, models):\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Model: {model_name}\")\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predictions on the test data\n",
    "        predictions_test = model.predict(X_test)\n",
    "\n",
    "        # Predictions on the train data\n",
    "        predictions_train = model.predict(X_train)\n",
    "\n",
    "        # Metrics for test data\n",
    "        accuracy_test = accuracy_score(y_test, predictions_test)\n",
    "        f1_test = f1_score(y_test, predictions_test, average='weighted')\n",
    "        recall_test = recall_score(y_test, predictions_test, average='weighted')\n",
    "\n",
    "        print(f\"Test Accuracy: {accuracy_test:.2f}\")\n",
    "        print(f\"Test F1 Score: {f1_test:.2f}\")\n",
    "        print(f\"Test Recall: {recall_test:.2f}\")\n",
    "        print(f\"Test Classification Report:\\n{classification_report(y_test, predictions_test)}\")\n",
    "\n",
    "        # Metrics for train data\n",
    "        accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "        f1_train = f1_score(y_train, predictions_train, average='weighted')\n",
    "        recall_train = recall_score(y_train, predictions_train, average='weighted')\n",
    "\n",
    "        print(f\"Train Accuracy: {accuracy_train:.2f}\")\n",
    "        print(f\"Train F1 Score: {f1_train:.2f}\")\n",
    "        print(f\"Train Recall: {recall_train:.2f}\")\n",
    "        print(f\"Train Classification Report:\\n{classification_report(y_train, predictions_train)}\")\n",
    "        list_model.append(model)\n",
    "        print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "        return list_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Splitting_Data(x,y):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "  return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fresh_data():\n",
    "  df=pd.read_csv(\"/content/generated_log_dataset_finale.csv\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000)\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# Initialize the vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1000)\n",
    "\n",
    "# Fit the vectorizer on the log data and transform the logs into TF-IDF features\n",
    "X_tfidf = vectorizer.fit_transform(df[\"log\"])\n",
    "\n",
    "# Initialize SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "\n",
    "# Fit SMOTEENN and resample the dataset\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X_tfidf, df[\"class\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=ml_models(Splitting_Data(X_resampled, y_resampled)[0], Splitting_Data(X_resampled, y_resampled)[1], Splitting_Data(X_resampled, y_resampled)[2], Splitting_Data(X_resampled, y_resampled)[3],models={    \"Logistic Regression\": LogisticRegression(max_iter=10000000),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Support Vector Machine\": SVC(),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=10, min_samples_split=10),\n",
    "    \"Random Forest\": RandomForestClassifier(max_depth=10, n_estimators=100),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    #\"Naive Bayes\": GaussianNB(),\n",
    "    \"Neural Network\": MLPClassifier(max_iter=10000, hidden_layer_sizes=(50, 50), learning_rate_init=0.01)\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(x[0],'model_finale.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
